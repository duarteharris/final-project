{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the work enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the plaidml backend\n",
    "## DO THIS BEFORE IMPORTING KERAS OR TENSOR TO USE PLAIDML\n",
    "import plaidml.keras\n",
    "\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help MacOS be able to use Keras\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Gets rid of the processor warning.\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' # haven't tried yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import argparse\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import keras\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "import re\n",
    "from scipy import stats as st\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display graphs inside notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version: 2.2.4\n",
      "nltk version: 3.4.5\n",
      "numpy version: 1.18.1\n",
      "pandas version: 1.0.1\n",
      "plaidml version: 0.7.0\n",
      "re version: 2.2.1\n",
      "Seaborn version: 0.10.0\n",
      "spacy version: 2.0.12\n"
     ]
    }
   ],
   "source": [
    "# Versions\n",
    "print(\"keras version:\", keras.__version__)\n",
    "print(\"nltk version:\", nltk.__version__)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"plaidml version:\", plaidml.__version__)\n",
    "print(\"re version:\", re.__version__)\n",
    "print(\"Seaborn version:\", sns.__version__)\n",
    "print(\"spacy version:\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories & Files\n",
    "os.listdir()\n",
    "\n",
    "# Datasets directory\n",
    "directory = \"./datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the DataFrames dynamically\n",
    "# 1st step: store the names and filenames of the files as a key-value pair in a dictionary\n",
    "datasets = {f\"{re.sub('.csv', '', filename.lower())}\": filename \n",
    "            for filename in os.listdir(directory)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the datasets dict\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleling '.ds_store': '.DS_Store' from the datasets dic\n",
    "del datasets[\".ds_store\"]\n",
    "\n",
    "# checking the datasets dic\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd step: for each key in the datasets dictionary, create a DF\n",
    "for name in datasets:\n",
    "    print(name)\n",
    "    globals()[name] = pd.read_csv(directory + datasets[name], encoding = \"ISO-8859-1\", \n",
    "                              error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the head\n",
    "ner_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different ways to check the % of nulls in a df/series\n",
    "print(\"Abs by Column:\\n\", ner_dataset.isna().sum())\n",
    "print(\"\\n% by Column:\\n\", ner_dataset.isna().mean().round(4) * 100)\n",
    "print(\"\\n% by Total:\", round((ner_dataset.isnull().any(axis = 1).sum() / len(ner_dataset)) \n",
    "                                * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking uniques in Sentence #\n",
    "ner_dataset[\"Sentence #\"].unique()\n",
    "\n",
    "# value counts in Sentence #\n",
    "ner_dataset[\"Sentence #\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after looking at it and reading about it, I figured that when Sentence # is NaN, the \n",
    "# corresponding Word in that row belongs to the previous sentence. As such, the foward fill\n",
    "# method is an approriate (ideal even?) way to deal with the NaN's.\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foward-filling NaN's in the dataset\n",
    "ner_dataset.fillna(method = \"ffill\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are still NaN's\n",
    "ner_dataset.isna().sum()\n",
    "ner_dataset.isnull().sum()\n",
    "\n",
    "# Checking the ner_dataset\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique tags value counts\n",
    "ner_dataset[\"Tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique tags value counts %\n",
    "ner_dataset[\"Tag\"].value_counts() / ner_dataset[\"Tag\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing and listing each unique word in the Word Series of the df\n",
    "words = list(set(ner_dataset[\"Word\"].values))\n",
    "\n",
    "# checking them\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (unique) word counter\n",
    "n_words = len(words) \n",
    "\n",
    "# cheking the word count\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"\n",
    "This ds has a total of 47 959 sentences, containing a total of {n_words} unique words.\n",
    "\n",
    "I think much of this (what comes ahead) can be better and more easily done with spaCy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# function to group the sentences by the POS (grammatical object) and Tag for each word.\n",
    "agg_func = lambda s: [(w, p, t) for w, p, t\n",
    "                      in zip(s[\"Word\"].values.tolist(),\n",
    "                            s[\"POS\"].values.tolist(),\n",
    "                            s[\"Tag\"].values.tolist())]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(JSON_FilePath):\n",
    "\n",
    "    training_data = list()\n",
    "    lines = list()\n",
    "        \n",
    "    with open(JSON_FilePath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data[\"content\"]\n",
    "        entities = []\n",
    "        \n",
    "        for annotation in data[\"annotation\"]:\n",
    "            #only a single point in text annotation.\n",
    "            point = annotation[\"points\"][0]\n",
    "            labels = annotation[\"label\"]\n",
    "                \n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "\n",
    "            for label in labels:\n",
    "                #these indices are both inclusive [start, end] but spacy is not \n",
    "                # [start, end)\n",
    "                #entities.append((point[\"start\"], point[\"end\"] + 1 ,label))\n",
    "                entities.append((point[\"start\"], point[\"end\"] + 1 ,label))\n",
    "\n",
    "\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_dataturks_to_spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4cebb29ad685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTRAIN_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_dataturks_to_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cv_traindata.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_dataturks_to_spacy' is not defined"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = convert_dataturks_to_spacy(directory + \"cv_traindata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcbf05f6c58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_DATA' is not defined"
     ]
    }
   ],
   "source": [
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 s, sys: 810 ms, total: 7.39 s\n",
      "Wall time: 7.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating a NLP object // Loading the model\n",
    "#nlp = spacy.load(\"en_core_web_sm\") # 11MB\n",
    "#nlp = spacy.load(\"en_core_web_md\") # 91MB\n",
    "nlp = spacy.load(\"en_core_web_lg\") # 789MB\n",
    "#nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the ner pipe, and adding it to the pipeline (if it's not there already)\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last = True)\n",
    "    \n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:00<00:23,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:29<00:00,  6.79it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 22648.764003465345}\n",
      "Statring iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:30<00:00,  6.58it/s]\n",
      "  0%|          | 1/200 [00:00<00:38,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 21429.838546635037}\n",
      "Statring iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:29<00:00,  6.68it/s]\n",
      "  0%|          | 1/200 [00:00<00:20,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 21101.109041517266}\n",
      "Statring iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [00:12<00:20,  5.93it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            \n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            \n",
    "            losses = {}\n",
    "            \n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop = 0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd = optimizer,  # callable to update weights\n",
    "                    losses = losses)\n",
    "                \n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model and evaluate it\n",
    "examples = convert_dataturks_to_spacy(directory + \"cv_testdata.json\")\n",
    "tp=0\n",
    "tr=0\n",
    "tf=0\n",
    "\n",
    "ta=0\n",
    "c=0        \n",
    "\n",
    "for text,annot in examples:\n",
    "    f=open(\"resume\"+str(c)+\".txt\",\"w\")\n",
    "    doc_to_test=nlp(text)\n",
    "    d={}\n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_]=[]\n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_].append(ent.text)\n",
    "\n",
    "    for i in set(d.keys()):\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(i +\":\"+\"\\n\")\n",
    "        for j in set(d[i]):\n",
    "            f.write(j.replace('\\n','')+\"\\n\")\n",
    "    d={}\n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_]=[0,0,0,0,0,0]\n",
    "    for ent in doc_to_test.ents:\n",
    "        doc_gold_text= nlp.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
    "        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
    "        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
    "        if(d[ent.label_][0]==0):\n",
    "            #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n",
    "            #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
    "            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "            a=accuracy_score(y_true,y_pred)\n",
    "            d[ent.label_][0]=1\n",
    "            d[ent.label_][1]+=p\n",
    "            d[ent.label_][2]+=r\n",
    "            d[ent.label_][3]+=f\n",
    "            d[ent.label_][4]+=a\n",
    "            d[ent.label_][5]+=1\n",
    "    c+=1\n",
    "for i in d:\n",
    "    print(\"\\n For Entity \"+i+\"\\n\")\n",
    "    print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
    "    print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
    "    print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
    "    print(\"F-score : \"+str(d[i][3]/d[i][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline component names\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Decide to start w/a pre-trained model an do tranfer learning, or build from scratch;\n",
    "\n",
    "1) Initialize the model weights randomly with nlp.begin_training;\n",
    "\n",
    "2) Predict a few examples with the current weights by calling nlp.update;\n",
    "\n",
    "3) Compare prediction with true labels;\n",
    "\n",
    "4) Calculate how to change weights to improve predictions;\n",
    "\n",
    "5) Update weights slightly;\n",
    "\n",
    "6) Go back to 2.\n",
    "\n",
    "---\n",
    "\n",
    "If we're not starting with a pre-trained model, we first initialize the weights randomly.\n",
    "\n",
    "---\n",
    "\n",
    "![title](training_in_spacy.svg)\n",
    "\n",
    "\n",
    "\n",
    "Training data: Examples and their annotations.\n",
    "\n",
    "Text: The input text the model should predict a label for.\n",
    "\n",
    "Label: The label the model should predict.\n",
    "\n",
    "Gradient: How to change the weights.\n",
    "\n",
    "---\n",
    "\n",
    "The entity recognizer tags words and phrases in context\n",
    "\n",
    "Each token can only be part of one entity\n",
    "\n",
    "Examples need to come with context\n",
    "\n",
    "(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]})\n",
    "\n",
    "Texts with no entities are also important\n",
    "\n",
    "(\"I need a new phone! Any tips?\", {'entities': []})\n",
    "\n",
    "Goal: teach the model to generalize\n",
    "\n",
    "---\n",
    "\n",
    "Because the entity recognizer predicts entities in context, it also needs to be trained on entities and their surrounding context.\n",
    "\n",
    "\n",
    "\n",
    "It's also very important for the model to learn words that aren't entities.\n",
    "\n",
    "In this case, the list of span annotations will be empty.\n",
    "\n",
    "Our goal is to teach the model to recognize new entities in similar contexts, even if they weren't in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop:\n",
    "The steps of a training loop:\n",
    "\n",
    "1) Loop for a number of times.\n",
    "\n",
    "2) Shuffle the training data.\n",
    "\n",
    "3) Divide the data into batches.\n",
    "\n",
    "4) Update the model for each batch.\n",
    "\n",
    "5) Save the updated model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We usually need to perform it several times, for multiple iterations, so that the model can learn from it effectively. If we want to train for 10 iterations, we need to loop 10 times.\n",
    "\n",
    "\n",
    "\n",
    "To prevent the model from getting stuck in a suboptimal solution, we randomly shuffle the data for each iteration. This is a very common strategy when doing stochastic gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "Next, we divide the training data into batches of several examples, also known as minibatching. This makes it easier to make a more accurate estimate of the gradient.\n",
    "\n",
    "\n",
    "\n",
    "Finally, we update the model for each batch, and start the loop again until we've reached the last iteration.\n",
    "\n",
    "\n",
    "\n",
    "We can then save the model to a directory and use it in spaCy.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The training data are the examples we want to update the model with.\n",
    "\n",
    "\n",
    "\n",
    "The text should be a sentence, paragraph or longer document. For the best results, it should be similar to what the model will see at runtime.\n",
    "\n",
    "\n",
    "\n",
    "The label is what we want the model to predict. This can be a text category, or an entity span and its type.\n",
    "\n",
    "\n",
    "\n",
    "The gradient is how we should change the model to reduce the current error. It's computed when we compare the predicted label to the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Best Case Practices\n",
    "\"\"\"\n",
    "Problem 1: Models can \"forget\" things\n",
    "\n",
    "— Existing model can overfit on new data\n",
    "e.g.: if you only update it with WEBSITE, it can \"unlearn\" what a PERSON is\n",
    "— Also known as \"catastrophic forgetting\" problem.\n",
    "\n",
    "If you're updating an existing model with new data, especially new labels, it can overfit \n",
    "and adjust too much to the new examples.\n",
    "\n",
    "Solution 1: Mix in previously correct predictions\n",
    "\n",
    "— For example, if you're training WEBSITE, also include examples of PERSON. \n",
    "— Run existing spaCy model over data and extract all other relevant entities\n",
    "\"\"\"\n",
    "# BAD:\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "]\n",
    "\n",
    "# GOOD:\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Problem 2: Models can't learn everything\n",
    "\n",
    "— spaCy's models make predictions based on local context\n",
    "— Model can struggle to learn if decision is difficult to make based on context\n",
    "— Label scheme needs to be consistent and not too specific\n",
    "For example: CLOTHING is better than ADULT_CLOTHING and CHILDRENS_CLOTHING\n",
    "\n",
    "for named entities, the surrounding words are most important.\n",
    "\n",
    "Solution 2: Plan your label scheme carefully\n",
    "\n",
    "— Pick categories that are reflected in local context\n",
    "— More generic is better than too specific\n",
    "— Use rules to go from generic labels to specific categories\n",
    "\n",
    "Before you start training and updating models, it's worth taking a step back and planning \n",
    "your label scheme.\n",
    "\n",
    "Try to pick categories that are reflected in the local context and make them more generic \n",
    "if possible.\n",
    "\n",
    "You can always add a rule-based system later to go from generic to specific.\n",
    "\"\"\"\n",
    "# BAD:\n",
    "LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
    "\n",
    "# GOOD:\n",
    "LABELS = ['CLOTHING', 'BAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building a training loop\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"exercises/gadgets.json\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size = 2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example loop\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples...\n",
    "]\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: creating patterns to quickly bootstrap some training data for our model.\n",
    "\"\"\"\n",
    "spaCy’s rule-based Matcher is a great way to quickly create training data for named entity \n",
    "models. A list of sentences is available as the variable TEXTS. You can print it the IPython\n",
    "shell to inspect it. We want to find all mentions of different iPhone models, so we can \n",
    "create training data to teach a model to recognize them as 'GADGET'.\n",
    "\"\"\"\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: using the match patterns created above to bootstrap a set of training examples.\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")\n",
    "\"\"\"\n",
    "Before you train a model with the data, you always want to double-check that your matcher \n",
    "didn't identify any false positives. But this process is still much faster than doing \n",
    "*everything* manually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "#from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# framework to build custom pipeline components\n",
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component) # first, last, before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for rule-based entity matching\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: \n",
    "\"\"\"\n",
    "create an attribute getter that returns a Wikipedia search URL if the span is a person, \n",
    "organization, or location\n",
    "\"\"\"\n",
    "from spacy.tokens import Span\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter = get_wikipedia_url) \n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best pratices:\n",
    "#BAD:\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "# GOOD:\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS)) # Much faster than calling nlp on each text\n",
    "\n",
    "#If you only need a tokenized Doc object, you can use the nlp dot make doc method instead, \n",
    "# which takes a text and returns a Doc.\n",
    "#BAD:\n",
    "doc = nlp(\"Hello world\")\n",
    "\n",
    "#GOOD:\n",
    "doc = nlp.make_doc(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling pipeline components\n",
    "#Use nlp.disable_pipes to temporarily disable one or more pipes\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "    \n",
    "\"\"\"\n",
    "After the with block, the disabled pipeline components are automatically restored.\n",
    "In the with block, spaCy will only run the remaining components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "example of efficient text processing. \n",
    "iterating over the doc objects yielded by nlp.pipe.\n",
    "\"\"\"\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])\"])\n",
    "    \n",
    "\"\"\"\n",
    "and:\n",
    "\"\"\"\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people)) # instead of patterns = [nlp(person) for person in people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a document to test\n",
    "doc = nlp(\"Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation, and published the first algorithm intended to be carried out by such a machine. As a result, she is sometimes regarded as the first to recognise the full potential of a 'computing machine' and one of the first computer programmers.[2][3][4]\"\n",
    "          \"Augusta Byron was the only legitimate child of poet Lord Byron and his wife Lady Byron.[5] All of Byron's other children were born out of wedlock to other women.[6] Byron separated from his wife a month after Ada was born and left England forever four months later. He commemorated the parting in a poem that begins, 'Is thy face like thy mother's my fair child! ADA! sole daughter of my house and heart?'.[7] He died of disease in the Greek War of Independence when Ada was eight years old. Her mother remained bitter and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in Byron, naming her two sons Byron and Gordon. Upon her eventual death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.2\"          \n",
    "          \"Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens, contacts which she used to further her education. Ada described her approach 'as poetical science'[8] and herself as an 'Analyst (& Metaphysician)'.[9]\"\n",
    "          \"When she was a teenager, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as 'the father of computers'. She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.\"\n",
    "          \"Between 1842 and 1843, Ada translated an article by Italian military engineer Luigi Menabrea on the calculating engine, supplementing it with an elaborate set of notes, simply called Notes. These notes contain what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine.[10] Lovelace's notes are important in the early history of computers. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities.[11] Her mindset of 'poetical science' led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.[6]\"\n",
    "          \"She died of uterine cancer in 1852 at the age of 36.\"\n",
    "          \"source: https://en.wikipedia.org/wiki/Ada_Lovelace\")\n",
    "\n",
    "# Finding entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# displaying the doc w/entities\n",
    "#displacy.serve(doc, style = \"ent\") # open http://localhost:5000\n",
    "\n",
    "# color and backgournd (bg) aren't working?\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "displacy.render(doc, style = \"ent\", jupyter = True, options = options) # to read directly in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lovelace = \"\"\"Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 \n",
    "November 1852) was an English mathematician and writer, chiefly known for her work on \n",
    "Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. \n",
    "She was the first to recognise that the machine had applications beyond pure calculation, \n",
    "and published the first algorithm intended to be carried out by such a machine. As a result, she is sometimes regarded as the first to recognise the full potential of a 'computing machine' and one of the first computer programmers.[2][3][4]\"\n",
    "Augusta Byron was the only legitimate child of poet Lord Byron and his wife Lady Byron.[5] \n",
    "All of Byron's other children were born out of wedlock to other women.[6] Byron separated \n",
    "from his wife a month after Ada was born and left England forever four months later. He \n",
    "commemorated the parting in a poem that begins, 'Is thy face like thy mother's my fair \n",
    "child! ADA! sole daughter of my house and heart?'.[7] He died of disease in the Greek War \n",
    "of Independence when Ada was eight years old. Her mother remained bitter and promoted Ada's \n",
    "interest in mathematics and logic in an effort to prevent her from developing her father's \n",
    "perceived insanity. Despite this, Ada remained interested in Byron, naming her two sons \n",
    "Byron and Gordon. Upon her eventual death, she was buried next to him at her request. \n",
    "Although often ill in her childhood, Ada pursued her studies assiduously. She married \n",
    "William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess \n",
    "of Lovelace.2          \n",
    "\n",
    "Her educational and social exploits brought her into contact with scientists such as Andrew \n",
    "Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday and the \n",
    "author Charles Dickens, contacts which she used to further her education. Ada described her \n",
    "approach 'as poetical science'[8] and herself as an 'Analyst (& Metaphysician)'.[9]\n",
    "\n",
    "When she was a teenager, her mathematical talents led her to a long working relationship \n",
    "and friendship with fellow British mathematician Charles Babbage, who is known as 'the \n",
    "father of computers'. She was in particular interested in Babbage's work on the Analytical \n",
    "Engine. Lovelace first met him in June 1833, through their mutual friend, and her private \n",
    "tutor, Mary Somerville.\n",
    "\n",
    "Between 1842 and 1843, Ada translated an article by Italian military engineer Luigi \n",
    "Menabrea on the calculating engine, supplementing it with an elaborate set of notes, \n",
    "simply called Notes. These notes contain what many consider to be the first computer \n",
    "program—that is, an algorithm designed to be carried out by a machine. Other historians \n",
    "reject this perspective and point out that Babbage's personal notes from the years \n",
    "1836/1837 contain the first programs for the engine.[10] Lovelace's notes are important in \n",
    "the early history of computers. She also developed a vision of the capability of computers \n",
    "to go beyond mere calculating or number-crunching, while many others, including Babbage \n",
    "himself, focused only on those capabilities.[11] Her mindset of 'poetical science' led her \n",
    "to ask questions about the Analytical Engine (as shown in her notes) examining how \n",
    "individuals and society relate to technology as a collaborative tool.[6]\n",
    "\n",
    "She died of uterine cancer in 1852 at the age of 36.\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Ada_Lovelace\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Redact Names\n",
    "def redact_names(text):\n",
    "    \"\"\"\n",
    "    take a text, convert it to an npl object, merge the id'd entities, check if the token is\n",
    "    type PERSON, if so, Redact, if not leave it be, and return the final text.\n",
    "    \"\"\"\n",
    "    docx = nlp(text)\n",
    "    redacted_sentences = list()\n",
    "    \"\"\"\n",
    "    for ent in docx.ents:\n",
    "        ent.merge()\n",
    "    \"\"\"\n",
    "    \n",
    "    [ent.merge() for ent in docx.ents]\n",
    "    \n",
    "    for token in docx:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentences += \"[REDACTED]\"\n",
    "        \n",
    "        else:\n",
    "            redacted_sentences += token.string\n",
    "            \n",
    "    return \"\".join(redacted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lovelace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redacting Names\n",
    "redact_names(lovelace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can I visualize it with the redaction? -> yes I can\n",
    "#displacy.render(nlp(redact_names(lovelace)), style = \"ent\", jupyter = True, options = options)\n",
    "\n",
    "displacy.serve(nlp(redact_names(lovelace)), style = \"ent\") # open http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_crf = joblib.load(\"my_crf_trained_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = spacy.load(trained_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
